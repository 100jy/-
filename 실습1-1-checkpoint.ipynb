{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twolayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "\n",
    "    x = x - np.max(x) \n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 \n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) \n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None \n",
    "        self.y = None    \n",
    "        self.t = None    \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "   \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        \n",
    "        self.loss(x, t)\n",
    "\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "       \n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"MNIST handwritten digits dataset.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "@keras_export('keras.datasets.mnist.load_data')\n",
    "def load_data(path='mnist.npz'):\n",
    "  \"\"\"Loads the MNIST dataset.\n",
    "\n",
    "  Arguments:\n",
    "      path: path where to cache the dataset locally\n",
    "          (relative to ~/.keras/datasets).\n",
    "\n",
    "  Returns:\n",
    "      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "\n",
    "  License:\n",
    "      Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n",
    "      which is a derivative work from original NIST datasets.\n",
    "      MNIST dataset is made available under the terms of the\n",
    "      [Creative Commons Attribution-Share Alike 3.0 license.](\n",
    "      https://creativecommons.org/licenses/by-sa/3.0/)\n",
    "  \"\"\"\n",
    "  origin_folder = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/'\n",
    "  path = get_file(\n",
    "      path,\n",
    "      origin=origin_folder + 'mnist.npz',\n",
    "      file_hash='8a61469f7ea1b51cbae51d4f78837e45')\n",
    "  with np.load(path) as f:\n",
    "    x_train, y_train = f['x_train'], f['y_train']\n",
    "    x_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.12555 test_acc: 0.1279 loss: 2.3014652174484054\n",
      "train_acc: 0.9043166666666667 test_acc: 0.9078 loss: 0.38388800419272956\n",
      "train_acc: 0.9219 test_acc: 0.9239 loss: 0.21202788934777841\n",
      "train_acc: 0.9295166666666667 test_acc: 0.929 loss: 0.16647800453199008\n",
      "train_acc: 0.9413833333333333 test_acc: 0.9389 loss: 0.20860773957610332\n",
      "train_acc: 0.94805 test_acc: 0.9448 loss: 0.12424403885441393\n",
      "train_acc: 0.95525 test_acc: 0.9509 loss: 0.09662478761435349\n",
      "train_acc: 0.9596666666666667 test_acc: 0.9563 loss: 0.17868713026007954\n",
      "train_acc: 0.96155 test_acc: 0.9581 loss: 0.15141294149285892\n",
      "train_acc: 0.9662 test_acc: 0.9605 loss: 0.07524392539152575\n",
      "train_acc: 0.9694 test_acc: 0.9641 loss: 0.11974409584301805\n",
      "train_acc: 0.97145 test_acc: 0.9669 loss: 0.06444257283419046\n",
      "train_acc: 0.973 test_acc: 0.967 loss: 0.16490154659366127\n",
      "train_acc: 0.9743 test_acc: 0.9679 loss: 0.07287525962619751\n",
      "train_acc: 0.9758833333333333 test_acc: 0.9684 loss: 0.030155611395166572\n",
      "train_acc: 0.9777166666666667 test_acc: 0.9689 loss: 0.03297611575049012\n",
      "train_acc: 0.97885 test_acc: 0.9706 loss: 0.038060734198961874\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "lr = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask =np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]  -= lr*grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train_acc:\",train_acc,\"test_acc:\", test_acc,\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(17)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기타등등.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    \n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = { 'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== 학습결과 ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "       \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "       \n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "       \n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "       \n",
    "      \n",
    "        self.loss(x, t)\n",
    "\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.492365300454148\n",
      "=== epoch:1, train acc:0.181, test acc:0.13 ===\n",
      "train loss:2.560183957584926\n",
      "train loss:2.658480359585872\n",
      "train loss:2.577131926992469\n",
      "train loss:2.6073236059272022\n",
      "train loss:2.666688648164091\n",
      "train loss:2.458242817483746\n",
      "train loss:2.4356552029145138\n",
      "train loss:2.4966610556523468\n",
      "train loss:2.611555817109736\n",
      "train loss:2.63692835560182\n",
      "train loss:2.6511468769983724\n",
      "train loss:2.592681752400076\n",
      "train loss:2.405644300126489\n",
      "train loss:2.6357808278083037\n",
      "train loss:2.487582847645022\n",
      "train loss:2.4885945878129396\n",
      "train loss:2.5500123589879977\n",
      "train loss:2.423114715566875\n",
      "train loss:2.5694499138518934\n",
      "train loss:2.312542357108413\n",
      "train loss:2.4440915283858615\n",
      "train loss:2.472571381883555\n",
      "train loss:2.46261648954431\n",
      "train loss:2.440527437804612\n",
      "train loss:2.445391785398266\n",
      "train loss:2.6236832597074256\n",
      "train loss:2.4630874879528815\n",
      "train loss:2.3338283874875536\n",
      "train loss:2.4242276486986256\n",
      "train loss:2.4142819773965694\n",
      "train loss:2.4039387129525833\n",
      "train loss:2.3184402994784743\n",
      "train loss:2.5448646387764255\n",
      "train loss:2.4262363227462087\n",
      "train loss:2.3216371743649664\n",
      "train loss:2.413085088753907\n",
      "train loss:2.446550375582085\n",
      "train loss:2.473982455120933\n",
      "train loss:2.3468743288482647\n",
      "train loss:2.523029823990183\n",
      "train loss:2.3599433628353164\n",
      "train loss:2.3920772556558636\n",
      "train loss:2.4927443389327015\n",
      "train loss:2.578588796970193\n",
      "train loss:2.325810168160226\n",
      "train loss:2.38865889840959\n",
      "train loss:2.3780884659244466\n",
      "train loss:2.37906147498648\n",
      "train loss:2.3780068227210225\n",
      "train loss:2.4533979416190204\n",
      "=== epoch:2, train acc:0.198, test acc:0.133 ===\n",
      "train loss:2.4041491460109734\n",
      "train loss:2.3890692612926174\n",
      "train loss:2.3786101055814397\n",
      "train loss:2.3297818984899923\n",
      "train loss:2.3234234364425586\n",
      "train loss:2.2527280461419963\n",
      "train loss:2.4108410908371734\n",
      "train loss:2.373646270327846\n",
      "train loss:2.217909296795472\n",
      "train loss:2.32917539914572\n",
      "train loss:2.2881451432956696\n",
      "train loss:2.262098778903229\n",
      "train loss:2.1993096783419137\n",
      "train loss:2.2691153972010865\n",
      "train loss:2.2417792082573413\n",
      "train loss:2.2655716371737964\n",
      "train loss:2.2113585867788927\n",
      "train loss:2.4047512976200505\n",
      "train loss:2.352601300677207\n",
      "train loss:2.2772637874120285\n",
      "train loss:2.424614114880079\n",
      "train loss:2.2989227702324313\n",
      "train loss:2.363206903474619\n",
      "train loss:2.4380364510567842\n",
      "train loss:2.2422861311857782\n",
      "train loss:2.295811854543489\n",
      "train loss:2.24996419962708\n",
      "train loss:2.2776941168949327\n",
      "train loss:2.292366800682794\n",
      "train loss:2.2356158275114044\n",
      "train loss:2.1531707193500815\n",
      "train loss:2.2658665414827435\n",
      "train loss:2.252705167951609\n",
      "train loss:2.306521892968249\n",
      "train loss:2.1809430635948286\n",
      "train loss:2.1940334406711983\n",
      "train loss:2.3262790400893945\n",
      "train loss:2.314562828777493\n",
      "train loss:2.276259682788961\n",
      "train loss:2.1868906563998203\n",
      "train loss:2.1684440763478836\n",
      "train loss:2.4176645331417728\n",
      "train loss:2.1590771668082054\n",
      "train loss:2.1890041156482076\n",
      "train loss:2.299150787284931\n",
      "train loss:2.296427886920029\n",
      "train loss:2.184419348139445\n",
      "train loss:2.2993326705390196\n",
      "train loss:2.243628085192375\n",
      "train loss:2.2425830006910403\n",
      "=== epoch:3, train acc:0.228, test acc:0.151 ===\n",
      "train loss:2.3058733723284557\n",
      "train loss:2.275938013558787\n",
      "train loss:2.147467311582882\n",
      "train loss:2.201131186161405\n",
      "train loss:2.1679869857913907\n",
      "train loss:2.283422177241214\n",
      "train loss:2.183998532501889\n",
      "train loss:2.1869699777524674\n",
      "train loss:2.1571235756277303\n",
      "train loss:2.1766435838893012\n",
      "train loss:2.0643391140488547\n",
      "train loss:2.21553525272857\n",
      "train loss:2.2388881986811953\n",
      "train loss:2.243804164510717\n",
      "train loss:2.1856066599562105\n",
      "train loss:2.2084632821774735\n",
      "train loss:2.2439753243964096\n",
      "train loss:2.123062901934843\n",
      "train loss:2.2215285320072913\n",
      "train loss:2.1241805318345888\n",
      "train loss:2.1573126078751055\n",
      "train loss:2.183340746974854\n",
      "train loss:2.245570954153949\n",
      "train loss:2.195052219298179\n",
      "train loss:2.222274131486778\n",
      "train loss:2.162377321987926\n",
      "train loss:2.137386013320585\n",
      "train loss:2.1739782951704383\n",
      "train loss:2.0828488435958157\n",
      "train loss:2.144879371858374\n",
      "train loss:2.2002542340282765\n",
      "train loss:2.213536170637983\n",
      "train loss:2.164983071808748\n",
      "train loss:2.115411096988834\n",
      "train loss:2.1370505297441094\n",
      "train loss:2.216111589051028\n",
      "train loss:2.09996221563261\n",
      "train loss:2.0924694182345274\n",
      "train loss:2.1461117062251205\n",
      "train loss:2.1926523761152805\n",
      "train loss:2.174575652924136\n",
      "train loss:2.1135747388632837\n",
      "train loss:2.1929822277202295\n",
      "train loss:2.1748089817910126\n",
      "train loss:2.1286762303863\n",
      "train loss:2.2075411238175984\n",
      "train loss:2.205371220342761\n",
      "train loss:2.087133248880332\n",
      "train loss:2.0176246738674632\n",
      "train loss:2.1718919812120867\n",
      "=== epoch:4, train acc:0.258, test acc:0.196 ===\n",
      "train loss:2.08404914869339\n",
      "train loss:2.1030492918186328\n",
      "train loss:2.1516566153548724\n",
      "train loss:2.1611989590318617\n",
      "train loss:2.0436971065380494\n",
      "train loss:2.111204923383291\n",
      "train loss:2.2062044513534356\n",
      "train loss:2.091004052211552\n",
      "train loss:2.0643264133778936\n",
      "train loss:2.04127342405373\n",
      "train loss:2.090171126738678\n",
      "train loss:2.0422974780527663\n",
      "train loss:2.028195609053939\n",
      "train loss:2.1887783654291026\n",
      "train loss:2.0420633032933613\n",
      "train loss:2.0769459112973867\n",
      "train loss:2.189805597832178\n",
      "train loss:2.1221399531346115\n",
      "train loss:2.122588077190107\n",
      "train loss:2.104503466342424\n",
      "train loss:2.0894939215360457\n",
      "train loss:1.9739320923072707\n",
      "train loss:2.0670053521094474\n",
      "train loss:2.0951473277848773\n",
      "train loss:2.002727777238447\n",
      "train loss:2.1929478546839154\n",
      "train loss:2.1073426356241516\n",
      "train loss:2.0326849796713464\n",
      "train loss:2.05170440785499\n",
      "train loss:2.0918040055266722\n",
      "train loss:2.0022572282498903\n",
      "train loss:2.0603863304555583\n",
      "train loss:2.119705332182053\n",
      "train loss:2.0328128611567924\n",
      "train loss:2.1080950175462903\n",
      "train loss:2.044168598486778\n",
      "train loss:2.121466734176075\n",
      "train loss:1.9712251706860073\n",
      "train loss:2.115658652093254\n",
      "train loss:2.0907091378408422\n",
      "train loss:2.047489199669437\n",
      "train loss:1.9314069232616236\n",
      "train loss:2.0342263500229443\n",
      "train loss:2.002819713056514\n",
      "train loss:2.0352146651132044\n",
      "train loss:2.0447669669296036\n",
      "train loss:1.9996491119193849\n",
      "train loss:2.063413222345827\n",
      "train loss:2.019629613211163\n",
      "train loss:2.0781505349219564\n",
      "=== epoch:5, train acc:0.294, test acc:0.244 ===\n",
      "train loss:1.9655434919847\n",
      "train loss:1.9922437860005158\n",
      "train loss:2.130408691617912\n",
      "train loss:1.9488548630092382\n",
      "train loss:2.0054098630683233\n",
      "train loss:1.9671711717159315\n",
      "train loss:2.022693461901587\n",
      "train loss:1.9138572634858522\n",
      "train loss:2.0182842379439414\n",
      "train loss:1.9111262584732998\n",
      "train loss:1.982626416157899\n",
      "train loss:1.9734888109239404\n",
      "train loss:1.9646618421507724\n",
      "train loss:2.0041539607244085\n",
      "train loss:2.0648920955163046\n",
      "train loss:1.9914592877149238\n",
      "train loss:1.9714881184979456\n",
      "train loss:2.1390775567443696\n",
      "train loss:1.9348743121013905\n",
      "train loss:2.022679548204303\n",
      "train loss:1.9687357949741102\n",
      "train loss:2.0338954379755076\n",
      "train loss:2.0833935671912793\n",
      "train loss:1.9112670984352704\n",
      "train loss:1.9472980479520916\n",
      "train loss:1.9772294304067668\n",
      "train loss:1.915238267830416\n",
      "train loss:1.9266946017022732\n",
      "train loss:1.9367385171714633\n",
      "train loss:2.018644776418477\n",
      "train loss:1.977822050591131\n",
      "train loss:2.0263308451518207\n",
      "train loss:2.0917146260632977\n",
      "train loss:1.9145697726463462\n",
      "train loss:2.022101802706863\n",
      "train loss:1.9456260497105942\n",
      "train loss:2.007203879217993\n",
      "train loss:1.900828522945723\n",
      "train loss:1.9266698636268955\n",
      "train loss:1.958321040614522\n",
      "train loss:1.9624271190129265\n",
      "train loss:1.9781355096710374\n",
      "train loss:1.994862288817507\n",
      "train loss:1.9196467163314646\n",
      "train loss:1.993150366812253\n",
      "train loss:1.913902807500437\n",
      "train loss:1.9510257936432125\n",
      "train loss:1.9212997505270746\n",
      "train loss:1.9439472359278291\n",
      "train loss:1.9968543164589119\n",
      "=== epoch:6, train acc:0.351, test acc:0.302 ===\n",
      "train loss:1.9788764316629843\n",
      "train loss:1.9633422215849583\n",
      "train loss:1.9769782219832017\n",
      "train loss:2.019484807601808\n",
      "train loss:1.9506539528621556\n",
      "train loss:1.9986614336259902\n",
      "train loss:1.9284282062872415\n",
      "train loss:1.8801113009929922\n",
      "train loss:1.9330104805697375\n",
      "train loss:1.8719259885112565\n",
      "train loss:1.900229164541373\n",
      "train loss:1.9795235089925531\n",
      "train loss:1.862998305692392\n",
      "train loss:1.998683857678539\n",
      "train loss:2.0245693394711277\n",
      "train loss:1.9195521202975425\n",
      "train loss:1.9182841374393538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.913117851773188\n",
      "train loss:1.9650724481673376\n",
      "train loss:1.92844204287736\n",
      "train loss:1.9176709434098662\n",
      "train loss:1.842993798308966\n",
      "train loss:1.9294644892864292\n",
      "train loss:2.014846778951864\n",
      "train loss:1.9850177551210328\n",
      "train loss:1.8889859066977601\n",
      "train loss:1.875597760870634\n",
      "train loss:1.8832083985344428\n",
      "train loss:1.8237908177821762\n",
      "train loss:2.033537059700468\n",
      "train loss:1.9367315717975506\n",
      "train loss:1.850561326197996\n",
      "train loss:1.936648970320505\n",
      "train loss:1.920742087981983\n",
      "train loss:1.8497152636890462\n",
      "train loss:1.8676234137232317\n",
      "train loss:1.8803018431640992\n",
      "train loss:1.804290882679516\n",
      "train loss:1.82613275893096\n",
      "train loss:1.915882417200381\n",
      "train loss:1.9079804701332106\n",
      "train loss:1.8317292597785704\n",
      "train loss:1.7832996117334823\n",
      "train loss:1.9050946316680095\n",
      "train loss:1.8789371291390835\n",
      "train loss:1.8291862586963723\n",
      "train loss:1.836085491831266\n",
      "train loss:1.8331566791969407\n",
      "train loss:1.8449410503506687\n",
      "train loss:1.9103583720684594\n",
      "=== epoch:7, train acc:0.406, test acc:0.374 ===\n",
      "train loss:1.8526297957252131\n",
      "train loss:1.9449490135874774\n",
      "train loss:1.8729306423887442\n",
      "train loss:1.8913628728333647\n",
      "train loss:1.870691937621466\n",
      "train loss:1.8607786778316264\n",
      "train loss:1.7428401629796557\n",
      "train loss:1.9009812023884598\n",
      "train loss:1.8045016617421838\n",
      "train loss:1.8906941926364877\n",
      "train loss:1.903213810027392\n",
      "train loss:1.9054564114093373\n",
      "train loss:1.8908779068045198\n",
      "train loss:1.8144933737742814\n",
      "train loss:1.865586423279427\n",
      "train loss:1.8453407597234592\n",
      "train loss:1.9325493527678903\n",
      "train loss:1.8107606458865917\n",
      "train loss:1.8695400427722828\n",
      "train loss:1.8394536920111508\n",
      "train loss:1.714766707566639\n",
      "train loss:1.8242728694041208\n",
      "train loss:1.942607238294053\n",
      "train loss:1.845354416849857\n",
      "train loss:1.8259027290016423\n",
      "train loss:1.8131136635002347\n",
      "train loss:1.839026126410924\n",
      "train loss:1.7502774176213316\n",
      "train loss:1.7527379277844712\n",
      "train loss:1.7651142537592535\n",
      "train loss:1.808153877121514\n",
      "train loss:1.7752632535943926\n",
      "train loss:1.7995391053248837\n",
      "train loss:1.8970788593897914\n",
      "train loss:1.773868472773599\n",
      "train loss:1.6945667240466735\n",
      "train loss:1.9031776298084244\n",
      "train loss:1.7974518106311421\n",
      "train loss:1.7708817166823503\n",
      "train loss:1.709448376139155\n",
      "train loss:1.8049218217752625\n",
      "train loss:1.77550482794866\n",
      "train loss:1.7633194443049418\n",
      "train loss:1.820086775062276\n",
      "train loss:1.7165905373761854\n",
      "train loss:1.8208649385985356\n",
      "train loss:1.8390647536266977\n",
      "train loss:1.8126292225882767\n",
      "train loss:1.7016523891315143\n",
      "train loss:1.7342942319411279\n",
      "=== epoch:8, train acc:0.441, test acc:0.431 ===\n",
      "train loss:1.8607040536248027\n",
      "train loss:1.8426360812175933\n",
      "train loss:1.7952050191313638\n",
      "train loss:1.892621991962169\n",
      "train loss:1.8227901489511837\n",
      "train loss:1.609685394938373\n",
      "train loss:1.7387721820856632\n",
      "train loss:1.8631707948560325\n",
      "train loss:1.6951950332863732\n",
      "train loss:1.7600959168165238\n",
      "train loss:1.7027515377770903\n",
      "train loss:1.7320241604357023\n",
      "train loss:1.8635628033041933\n",
      "train loss:1.7264364820474298\n",
      "train loss:1.760607234342392\n",
      "train loss:1.8519591991549282\n",
      "train loss:1.7120048410884723\n",
      "train loss:1.7454298022258923\n",
      "train loss:1.7850026964465098\n",
      "train loss:1.653409197865516\n",
      "train loss:1.7239229861551502\n",
      "train loss:1.8656289414803768\n",
      "train loss:1.6656602957484754\n",
      "train loss:1.6440302780411236\n",
      "train loss:1.7285876999530527\n",
      "train loss:1.6879154692469447\n",
      "train loss:1.7385427128513506\n",
      "train loss:1.7075214778350947\n",
      "train loss:1.7810701859695555\n",
      "train loss:1.7277807435353671\n",
      "train loss:1.764864958178552\n",
      "train loss:1.7575827113851694\n",
      "train loss:1.6618686400710465\n",
      "train loss:1.804990845254797\n",
      "train loss:1.644248545045948\n",
      "train loss:1.7427790468310929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-230-f01b7b87c5c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SGD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-185-d1c2759cdd5f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-185-d1c2759cdd5f>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train loss:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-186-f5f75484a0a2>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-186-f5f75484a0a2>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-184-23eda57ccd9e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0marg_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2503\u001b[0m     \"\"\"\n\u001b[0;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[1;32m-> 2505\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=False, one_hot_label=True)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.1)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# mnist.py\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('You should use Python 3.x')\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = \"C:/Users/wnduq/.ipynb_checkpoints\"\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "        \n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"Creating pickle file ...\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"MNIST 데이터셋 읽기\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
    "    one_hot_label : \n",
    "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
    "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
    "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
